{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1219980802.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    model_trainer:\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model_trainer:\n",
    "   root_dir: artifacts/model_trainer\n",
    "   data_path: artifacts/data_transformation/samsum-test\n",
    "   model_ckpt: google/pegasus-cnn_dailymail \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.entity import *\n",
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, get_size, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path = CONFIG_FILE_PATH, params_path = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_modeltrainer_config(self)-> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        \n",
    "        params = self.params.TrainingArguments # training parameters\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config  = ModelTrainerConfig(\n",
    "             root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            warmup_steps = params.warmup_steps,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            weight_decay = params.weight_decay,\n",
    "            logging_steps = params.logging_steps,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            eval_steps = params.evaluation_strategy,\n",
    "            save_steps = params.save_steps,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_pegasus)\n",
    "\n",
    "        #load_data\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        # Split into train and validation manually\n",
    "        dataset_samsum_pt = dataset_samsum_pt['train'].train_test_split(test_size=0.1) # 90% train, 10% validation\n",
    "\n",
    "\n",
    "        # Training Arguments\n",
    "        # train_args = TrainingArguments( \n",
    "        #     output_dir=self.config.root_dir,  # to save the output in the model trainer folder\n",
    "        #     num_train_epochs= self.config.num_train_epochs,\n",
    "        #     warmup_steps=self.config.warmup_steps,\n",
    "        #     per_device_train_batch_size =self.config.per_device_train_batch_size,\n",
    "        #     weight_decay = self.config.weight_decay,\n",
    "        #     logging_steps= self.config.logging_steps,\n",
    "        #     evaluation_strategy= self.config.evaluation_strategy,\n",
    "        #     eval_steps= self.config.eval_steps,\n",
    "        #     save_steps = self.config.eval_steps,\n",
    "        #     gradient_accumulation_steps = self.config.gradient_accumulation_steps)\n",
    "  \n",
    "\n",
    "        \n",
    "        train_args = TrainingArguments(\n",
    "            output_dir=str(self.config.root_dir), num_train_epochs=1, warmup_steps=500,\n",
    "            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "            weight_decay=0.01, logging_steps=10,\n",
    "            evaluation_strategy='steps', eval_steps=500, save_steps=1e6, #1000000\n",
    "            gradient_accumulation_steps=16,  fp16=True,  # <--- this saves 50% memory\n",
    "            dataloader_pin_memory=False  # <--- small trick to reduce GPU RAM\n",
    "        )\n",
    "\n",
    "        #To train \n",
    "        trainer = Trainer(model=model_pegasus,  \n",
    "                          tokenizer=tokenizer, \n",
    "                          args=train_args,\n",
    "                          data_collator=seq2seq_data_collator,\n",
    "                train_dataset=dataset_samsum_pt['train'], \n",
    "                eval_dataset=dataset_samsum_pt['test'])\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        #save model and tokenizer\n",
    "        model_pegasus.save_pretrained(os.path.join(str(self.config.root_dir), 'pegasus_samsum_model'))\n",
    "        tokenizer.save_pretrained(os.path.join(str(self.config.root_dir), 'tokenizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_pegasus)\n",
    "\n",
    "        # Load dataset\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        print(\"dataset_samsum_pt--\", dataset_samsum_pt)\n",
    "\n",
    "        # Split into train and validation manually\n",
    "        dataset_samsum_pt = dataset_samsum_pt['train'].train_test_split(test_size=0.1)  # 90% train, 10% validation\n",
    "        print(\"dataset_samsum_pt ['train']--\", dataset_samsum_pt['train'])\n",
    "\n",
    "        # Training Arguments\n",
    "        train_args = TrainingArguments(\n",
    "            output_dir=str(self.config.root_dir), \n",
    "            num_train_epochs=1, warmup_steps=50,\n",
    "            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "            weight_decay=0.01, logging_steps=10,\n",
    "            evaluation_strategy='steps', eval_steps=50, save_steps=1e6, #1000000\n",
    "            gradient_accumulation_steps=16,  fp16=True,  # <--- this saves 50% memory\n",
    "            dataloader_pin_memory=False  # <--- small trick to reduce GPU RAM\n",
    "        )\n",
    "        \n",
    "        # print(\"train_args --\", train_args)\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            tokenizer=tokenizer,\n",
    "            args=train_args,\n",
    "            data_collator=seq2seq_data_collator,\n",
    "            train_dataset=dataset_samsum_pt['train'],\n",
    "            eval_dataset=dataset_samsum_pt['test']\n",
    "        )\n",
    "\n",
    "        print(\"trainer---\", trainer)\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        model_pegasus.save_pretrained(os.path.join(str(self.config.root_dir), 'pegasus_samsum_model'))\n",
    "        tokenizer.save_pretrained(os.path.join(str(self.config.root_dir), 'tokenizer'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 16:19:09,561: INFO: common: created directory at: artifacts]\n",
      "[2025-04-26 16:19:09,563: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ananconda\\envs\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_samsum_pt-- Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 819\n",
      "})\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'label']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model_trainer_config \u001b[38;5;241m=\u001b[39m cofig\u001b[38;5;241m.\u001b[39mget_modeltrainer_config()\n\u001b[0;32m      4\u001b[0m model_trainer  \u001b[38;5;241m=\u001b[39m ModelTrainer(config\u001b[38;5;241m=\u001b[39mmodel_trainer_config)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 21\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_samsum_pt--\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_samsum_pt)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Split into train and validation manually\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m dataset_samsum_pt \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_samsum_pt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# 90% train, 10% validation\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_samsum_pt [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]--\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_samsum_pt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m dataset_split \u001b[38;5;241m=\u001b[39m dataset_samsum_pt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32md:\\Ananconda\\envs\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2777\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2776\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ananconda\\envs\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2761\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2759\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2760\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2761\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2762\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2763\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2764\u001b[0m )\n\u001b[0;32m   2765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32md:\\Ananconda\\envs\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:609\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    607\u001b[0m         _raise_bad_key_type(key)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 609\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[1;32md:\\Ananconda\\envs\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:546\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[1;34m(key, columns)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'label']\""
     ]
    }
   ],
   "source": [
    "cofig = ConfigurationManager()\n",
    "model_trainer_config = cofig.get_modeltrainer_config()\n",
    "\n",
    "model_trainer  = ModelTrainer(config=model_trainer_config)\n",
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
